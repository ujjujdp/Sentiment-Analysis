{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-VOBcURXBvh"
      },
      "outputs": [],
      "source": [
        "# Importing necessary packages\n",
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8GSp7yXXM6d",
        "outputId": "43628252-cf3a-488d-ce1a-b9c0c48c20c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainPath = \"/content/drive/MyDrive/IISc/DLNLP/Assignment2/train.csv\"\n",
        "trainData = pd.read_csv(trainPath)\n",
        "trainData = trainData.values.tolist()"
      ],
      "metadata": {
        "id": "GQSVBI0kXPPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def posTo1Negto0(tempStr):\n",
        "  if(tempStr == \"positive\"):\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "trainData = np.array(trainData)[:500]\n",
        "X_sentences = trainData[:,0]\n",
        "y = [posTo1Negto0(x) for x in trainData[:,1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "a-0T9Bj1a5D1",
        "outputId": "886e2b13-d833-4954-cf84-5ac9f4fce34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1bb444eff3d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'apply'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainData.shape)\n",
        "print(X_sentences[4])\n",
        "print(y[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW2zTCY9a5kG",
        "outputId": "fb16d16e-a3f7-46f8-a4b8-d843dada47af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 2)\n",
            "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for sentence preprocessing"
      ],
      "metadata": {
        "id": "uc85pf1GCliP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remPunc(text):\n",
        "  if type(text) == str:\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    text = re.sub('[\\W]+', '', text.lower())\n",
        "    return text\n",
        "\n",
        "def refineSentence(tempSentence):\n",
        "  tokens = nlp(str(tempSentence))\n",
        "  filtered = []\n",
        "  for token in tokens:\n",
        "    if (token.is_stop == False):\n",
        "      textAfterRemPunc = remPunc(token.lemma_)\n",
        "      if(textAfterRemPunc != ''):\n",
        "        filtered.append(textAfterRemPunc)\n",
        "  return filtered "
      ],
      "metadata": {
        "id": "9NhmOdJKCRKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PreProcessing the Train Data"
      ],
      "metadata": {
        "id": "ZBRHs_PbC2wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preProccessedList = [refineSentence(sentence) for sentence in X_sentences]"
      ],
      "metadata": {
        "id": "nZGEQ-IOC1s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preProccessedList[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DGlLUv0C_D4",
        "outputId": "cc9e215f-16bc-4715-d9da-5b866e2f612e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reviewer', 'mention', 'watch', '1', 'oz', 'episode', 'hook', 'right', 'exactly', 'happen', 'mebr', 'br', 'the', 'thing', 'strike', 'oz', 'brutality', 'unflinche', 'scene', 'violence', 'set', 'right', 'word', 'trust', 'faint', 'hearted', 'timid', 'pull', 'punch', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic', 'use', 'wordbr', 'br', 'it', 'call', 'oz', 'nickname', 'give', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focus', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inward', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'aryans', 'muslims', 'gangsta', 'latinos', 'christians', 'italians', 'irish', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'far', 'awaybr', 'br', 'i', 'main', 'appeal', 'fact', 'go', 'show', 'dare', 'forget', 'pretty', 'picture', 'paint', 'mainstream', 'audience', 'forget', 'charm', 'forget', 'romance', 'oz', 'mess', 'episode', 'see', 'strike', 'nasty', 'surreal', 'ready', 'watch', 'develop', 'taste', 'oz', 'get', 'accustomed', 'high', 'level', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guard', 'sell', 'nickel', 'inmate', 'kill', 'order', 'away', 'mannered', 'middle', 'class', 'inmate', 'turn', 'prison', 'bitch', 'lack', 'street', 'skill', 'prison', 'experience', 'watching', 'oz', 'comfortable', 'uncomfortable', 'viewing', 'that', 'touch', 'dark']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting GloVe Embeddings"
      ],
      "metadata": {
        "id": "R2rYV2wbIQAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddingsDict = {}\n",
        "dim = 300\n",
        "with open(\"/content/drive/MyDrive/IISc/DLNLP/Assignment1/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as wordEmbeddings:\n",
        "  for line in wordEmbeddings:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:],'float32')\n",
        "    embeddingsDict[word]=vector"
      ],
      "metadata": {
        "id": "mrehafRAIQu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert refined sentence to average sentence vector"
      ],
      "metadata": {
        "id": "jnl8Zb6XICg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentenceToWordEmbeddings(preProccessedList):\n",
        "  preProccessedListVectors = []\n",
        "  for preProcessedWordList in preProccessedList:\n",
        "    tempList = []\n",
        "    for word in preProcessedWordList:\n",
        "      if(word in embeddingsDict):\n",
        "        tempList.append(embeddingsDict[word])\n",
        "    preProccessedListVectors.append(np.array(tempList, dtype = float))\n",
        "  return preProccessedListVectors"
      ],
      "metadata": {
        "id": "AD9qJkKhID3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Data to wordsVector Matrix"
      ],
      "metadata": {
        "id": "qzuunpUsHxSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordsVector = sentenceToWordEmbeddings(preProccessedList)"
      ],
      "metadata": {
        "id": "vjmkKC68Hl_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsVector = torch.tensor(wordsVector, dtype = float)\n",
        "print(wordsVector[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "pYkTyoiPmW3N",
        "outputId": "cd369829-2c87-4d74-a085-13edb4fdcfdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-730f646eec96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordsVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsVector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 143 at dim 1 (got 84)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, num_layers, hidden_size, num_classes):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.LSTM = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bias = True, batch_first = False, dropout = 0, bidirectional = False )\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    self.softmax=nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to_device\n",
        "\n",
        "    out, _ = self.LSTM(x, h0)\n",
        "    out = out[:,-1,:]\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "5tR_nfpbmt2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SA_LSTM:\n",
        "  def __init__(self, input_size, num_layers, hidden_size, num_classes, epochs,lr):\n",
        "    self.input_size = input_size\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_classes = num_classes\n",
        "    self.epochs = epochs\n",
        "    \n",
        "    self.model = LSTM(input_size, num_layers, hidden_size, num_classes)\n",
        "\n",
        "    self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "    self.optimizer = optim.SGD(self.NNobj.parameters(), lr = lr)\n",
        "    \n",
        "    self.trainLossList = []\n",
        "    self.validLossList = []\n",
        "    self.trainLossListPlotting = []\n",
        "    self.validLossListPlotting = []\n",
        "\n",
        "  def fit(self, train_data, valid_data):\n",
        "    for i in range(self.epochs):\n",
        "      self.trainLossList = []\n",
        "      self.validLossList = []\n",
        "      accuracyListTrain = []\n",
        "      for batch in train_data:\n",
        "        self.model.train()                         #does dropout\n",
        "        x, y = batch\n",
        "        # b = x.size(0)\n",
        "        # x = x.view(b, -1)\n",
        "\n",
        "        forwardValue = self.model(x)\n",
        "        costFunction_J = self.cross_entropy_loss(forwardValue, y)\n",
        "        self.NNobj.zero_grad()\n",
        "        costFunction_J.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trainLossList.append(costFunction_J.item())\n",
        "        accuracyListTrain.append(y.eq(forwardValue.detach().argmax(dim = 1)).float().mean())\n",
        "          \n",
        "      accuracyListVal = []\n",
        "      for batch in valid_data:\n",
        "        self.NNobj.eval()                           ##stops dropout\n",
        "        x, y = batch\n",
        "        with torch.no_grad():\n",
        "          forwardValue = self.model(x)\n",
        "          costFunction_J = self.cross_entropy_loss(forwardValue, y)\n",
        "        self.validLossList.append(costFunction_J.item())\n",
        "        accuracyListVal.append(y.eq(forwardValue.detach().argmax(dim = 1)).float().mean())\n",
        "      \n",
        "      self.trainLossListPlotting.append(torch.tensor(self.trainLossList).mean())\n",
        "      self.validLossListPlotting.append(torch.tensor(self.validLossList).mean())\n",
        "      accuracyTrain = torch.tensor(accuracyListTrain).mean() * 100\n",
        "      accuracyVal = torch.tensor(accuracyListVal).mean() * 100\n",
        "\n",
        "      print('At Epoch Number: ' + str(i+1) +'; Train Loss= ' + str(\"{:.2f}\".format(torch.tensor(self.trainLossList).mean()))+'; Validation Loss= ' + str(\"{:.2f}\".format(torch.tensor(self.validLossList).mean())) + \"; \" + \"Train Accuracy = \", \"{:.2f}\".format(accuracyTrain), \"%\" + \"; \" + \"Validation Accuracy = \", \"{:.2f}\".format(accuracyVal), \"%\")\n",
        "\n",
        "  def predict(self,testData):\n",
        "    accuracyListPred = []\n",
        "    for batch in testData:\n",
        "      x, y = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "        forwardValue = self.NNobj(x)\n",
        "          \n",
        "      accuracyListPred.append(y.eq(forwardValue.detach().argmax(dim = 1)).float().mean())\n",
        "    accuracy = torch.tensor(accuracyListPred).mean() * 100\n",
        "    print('\\033[1m' + \"Review Classification Accuracy on Test Data = \", \"{:.2f}\".format(accuracy), \" %\" + '\\033[0m' )\n",
        "\n",
        "  def plotLossCurve(self, flag):\n",
        "    x = [(i+1) for i in range(self.epochs)]\n",
        "    plt.xlabel('#Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    tempStr = \"Loss curve for \"\n",
        "    if(flag == 0):\n",
        "        tempStr += \"Train Data\"\n",
        "        plt.plot(x,self.trainLossListPlotting)\n",
        "    else:\n",
        "        tempStr += \"Validation Data\"\n",
        "        plt.plot(x,self.validLossListPlotting)\n",
        "    plt.title(tempStr)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Qy4tjuvrqYga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "hidden_size = 512\n",
        "input_size = dim\n",
        "seq_size = dim\n",
        "num_classes = 2\n",
        "num_layers = 2\n",
        "lr = 1e-2"
      ],
      "metadata": {
        "id": "KL8s3NQ4mwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = SA_LSTM(input_size, num_layers, hidden_size, num_classes, epochs,lr)\n",
        "obj.fit(train_dataloader, valid_dataloader)\n",
        "obj.plotLossCurve(0)\n",
        "obj.plotLossCurve(1)"
      ],
      "metadata": {
        "id": "CggZ2laXr1XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj.predict(test_dataloader)"
      ],
      "metadata": {
        "id": "-gBuxOM8sSRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}